{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3eG2bT5riJl"
   },
   "source": [
    "# Excercise 2: Pre-processing and data augmentation\n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/Ex2_intro.jpg)\n",
    "\n",
    "Is this a male or female? You likely have no trouble recognizing this without effort. But what about a computer, could we teach it to recognize this? Of course we're talking biological, appearance based gender, not how a person chooses to identify. How someone identifies would be practically impossible even for us humans to determine based on a picture alone, let alone for a computer!\n",
    "\n",
    "----\n",
    "\n",
    "In this notebook we'll use the above classification task as a guiding example. We'll give you only a little data. The notebook will guide you through the pre-processing you can perform on your dataset prior to applying machine/deep learning. Pre-processing often can increase model performance significantly, and sometimes it is absolutely necessary for your models to learn anything at all.\n",
    "\n",
    "There is an optional exercise at the end dealing with data augmentation. As we've not given you enough data to learn your model properly, what else can we do? This gives some ideas for what to do when you have little data. Sometimes you can create a 'virtual expansion' of your dataset by augmenting it with variations on existing data. This helps increase your dataset and helps create a more robust model in the end.\n",
    "\n",
    "You will learn at least:\n",
    "- Handling variable input sizes (2.1)\n",
    "- Normalizing or standardizing your data (2.2)\n",
    "- OPTIONAL: How can you augment your set to improve model performance (and should you)? (2.3)\n",
    "- OPTIONAL: How do you split your data for training the deep learning model? What do you take into account (2.4)?\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULuSv70priJm"
   },
   "source": [
    "-------------\n",
    "    \n",
    "**First, run the cell below to import the required packages we'll be using**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "SBOzghzTriJm",
    "outputId": "386a6ce1-f0b2-4325-fa8a-fed78dd6636b"
   },
   "outputs": [],
   "source": [
    "#download required datasets for this notebook (might take a bit, be patient!)\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download(url, file):\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)\n",
    "    if not os.path.isfile(file):\n",
    "        print(\"Download file... \" + file + \" ...\")\n",
    "        urlretrieve(url,file)\n",
    "        print(\"File downloaded\")\n",
    "        \n",
    "def unzip(file):\n",
    "    with ZipFile(file) as f:\n",
    "        f.extractall()\n",
    "    print('unzipped file: %s\\n' %file)\n",
    "\n",
    "try:\n",
    "    download('http://www.paulvangent.com/files/DL_Course/celeba_larger.zip', 'celeba_larger.zip')\n",
    "except:\n",
    "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754594&authkey=AIcRTaFQqi0WnIo', 'celeba_larger.zip')\n",
    "unzip('celeba_larger.zip')\n",
    "\n",
    "try:\n",
    "    download('http://www.paulvangent.com/files/DL_Course/misc_day2.zip', 'misc_day2.zip')\n",
    "except:\n",
    "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754608&authkey=AOJO5mcV9eCZsJ8', 'misc_day2.zip')\n",
    "unzip('misc_day2.zip')\n",
    "\n",
    "try:\n",
    "    download('http://www.paulvangent.com/files/DL_Course/test_images.zip', 'test_images.zip')\n",
    "except:\n",
    "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754595&authkey=AETVSL8bPBIpTnI', 'test_images.zip')\n",
    "unzip('test_images.zip')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.ndimage import rotate\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "\n",
    "import utils_day2 as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z47PZNY9riJp"
   },
   "source": [
    "## 2.1 - Variable input sizes\n",
    "\n",
    "Usually the first step in any deep learning or machine learning project is getting your data ready. This means pre-processing your data and putting it in the appropriate container.\n",
    "\n",
    "For these examples we'll use an excerpt of 2000 images from the CelebA dataset, which contains annotations for facial landmarks and a range of attributes, such as whether the person in the image is wearing a hat, has a beard, is attractive, etc.\n",
    "\n",
    "For example, consider one random female and one random male entry from the set with attached annotated labels, and the annotated bounding box for the face overlaid:\n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/celeba_1.jpg)\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/celeba_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqVWhEOdriJq"
   },
   "source": [
    "However, before we can start training a model, there is a problem to fix. Every image in the set has different dimensions! While you can make a network work with variable input dimensions, because of the way the data is aggregated and passed through the network during training, at least the dimensions of all images in a given batch needs to be the same.\n",
    "\n",
    "For now we'll train the network to handle images of a set size, because variable sizes will introduce other caveats that go beyond what we'll be doing today.\n",
    "\n",
    "---------\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Resize all images you loaded into memory to 100x100 pixels and put them into the numpy array we initialize below\n",
    "\n",
    "- Try not to distort the images, think about how you can approach this and break into smaller problems\n",
    "    \n",
    "    hint:\n",
    "    - the largest dimension needs to become 100\n",
    "    - so you need to compute with what factor you need to multiply the dimensions so that the largest becomes 100 \n",
    "\n",
    "[resizing hint](https://docs.opencv.org/3.0-alpha/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv2.resize), note the dimensions need to be integers\n",
    "\n",
    "Second hint, a visualisation of what you need to do:\n",
    "![resize and put in container](http://www.paulvangent.com/files/DL_Course/day2_images/Zero-pad.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "iuPPiSyLriJq",
    "outputId": "e355ccd3-ab16-4f26-ccc6-4072d12438c2"
   },
   "outputs": [],
   "source": [
    "#get the data and labels\n",
    "labels = pd.read_csv('celeba_larger/labels/annotations.txt')\n",
    "#files are specified in the annotations\n",
    "filelist = labels.values[:,0]\n",
    "Y_data = labels['Male'].values\n",
    "\n",
    "#first we define a numpy array to hold all image data\n",
    "X_data= np.zeros(shape=(2000, 100, 100, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(filelist)):\n",
    "    img = cv2.imread('celeba_larger/data/%s' %filelist[i])\n",
    "    \n",
    "    ###Start of coding segment###\n",
    "    \n",
    "    #Complete the code below by following the steps outlined\n",
    "    #get largest dimension\n",
    "    largest_dim = \n",
    "    \n",
    "    #compute with what factor we need to  multiply both dimensions\n",
    "    resize_factor = \n",
    "    \n",
    "    #compute new dimensions\n",
    "    x_dim = \n",
    "    y_dim = \n",
    "    \n",
    "    #resize image\n",
    "    img_resized = cv2.resize(...)\n",
    "    \n",
    "    #put into container\n",
    "    X_data[i][0:img_resized.shape[0], 0:img_resized.shape[1]] = img_resized\n",
    "    \n",
    "    ###End of coding segment###\n",
    "    \n",
    "#visualise a random image\n",
    "plt.imshow(X_data[np.random.randint(0, len(X_data))][...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPE0QgncriJu"
   },
   "source": [
    "## 2.2 - Normalizing or Standardizing your training data\n",
    "\n",
    "We can almost start training our network! There is one last thing you usually do before training. In the exercises yesterday after loading the data, the first thing we did was normalize the input values. But why?\n",
    "\n",
    "Remember from the lecture that for each layer in the network, its input is the output of the previous layer. Each layer will multiply its input with a series of weights, pass it through some activation function, and pass the output to the next layer.\n",
    "\n",
    "**What do you think, could not normalizing the data be a problem? Why (not)?**\n",
    "\n",
    "**Would the problem change if we use other data than images?**\n",
    "\n",
    "-------------\n",
    "**Exercise**\n",
    "\n",
    "Let's run a little empirical exercise to warm up and see what happens when not normalizing and when normalizing. Let's try predicting gender from the CelebA subset you loaded and resized.\n",
    "\n",
    "Complete the cell below to load up the convnet you designed yesterday and compile it.\n",
    "\n",
    "- use binary crossentropy loss\n",
    "- use the adamax optimizer\n",
    "\n",
    "\n",
    "Hint: look at how we compiled the model yesterday, or [have a look at the Keras API](https://keras.io/models/model/#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "MlUb-EOlriJv",
    "outputId": "00139afe-c940-4a95-96da-1d02beaaa469",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "from misc import convnet\n",
    "model = convnet.build_model(input_shape=(100, 100, 3), classes=1)\n",
    "\n",
    "#split into training and validation set (more on this in 2.4)\n",
    "X_train = X_data[0:1800]\n",
    "X_val = X_data[1800:]\n",
    "\n",
    "Y_train = Y_data[0:1800]\n",
    "Y_val = Y_data[1800:]\n",
    "\n",
    "###Start of coding segment###\n",
    "#Add your code here\n",
    "model.compile(...)\n",
    "\n",
    "###End of coding segment###\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ya-bKyHpriJx"
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Train the model you just compiled on non-normalized data for 5 epochs and evaluate the model performance. Let's start easy and see if we can teach the model to learn whether the image if male or female.\n",
    "\n",
    "- Use X_train as training set and Y_train as training labels\n",
    "- Shuffle the data between epochs\n",
    "- Make sure to leave 'nonnorm_hist =' in front of the model.fit() call. We'll use this later on to plot progress\n",
    "- 5 epochs should be enough, but feel free to play around!\n",
    "\n",
    "Hint: look at how you did this yesterday, or [have a look at the Keras API](https://keras.io/models/model/#fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "V6lbEi0qriJy",
    "outputId": "0cd5adec-fda1-41d6-edcd-4f9e4d22bb4a"
   },
   "outputs": [],
   "source": [
    "#Add your code here (approx 1 line)\n",
    "nonnorm_hist = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiXxg6q8riJ0"
   },
   "source": [
    "- **Take a moment to look at the results. Describe what happened.**\n",
    "- **Did the model learn anything at all? If so, are you satisfied with the results?**\n",
    "- **Did the model learn anything at all?**\n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/XKCD_shufflepile.png)\n",
    "\n",
    "### Why Normalize your inputs?\n",
    "\n",
    "Normalizing data can help your model optimizer to converge much better and faster to a global/local minimum. Without normalized inputs this can take a lot longer, or even prove impossible as you have just observed.\n",
    "\n",
    "Let's confirm this with an empirical test. \n",
    "\n",
    "---------\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Complete the cell below with code to normalize the dataset, then run it to observe the behaviour of the same model on normalized data.\n",
    "\n",
    "- Normalize all image data.\n",
    "- Use same model compiling parameters.\n",
    "- Run the model for 10 epochs (we suspect this one *just might* learn something..!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1333
    },
    "colab_type": "code",
    "id": "dc73DZkUriJ0",
    "outputId": "e5e57576-c8c0-4ba9-aa5e-ba4bf98d14ee"
   },
   "outputs": [],
   "source": [
    "###Start of coding segment###\n",
    "#Complete the code below to normalize the data (approx. 2 lines)\n",
    "X_train_normalized = \n",
    "X_val_normalized = \n",
    "###End of coding segment###\n",
    "\n",
    "#Re-initialize a blank model and compile it again\n",
    "model = convnet.build_model(input_shape=(100, 100, 3), classes=1)\n",
    "\n",
    "###Start of coding segment###\n",
    "#Compile the model with binary crossentropy loss and adamax optimizer\n",
    "...\n",
    "\n",
    "#Complete the code below to fit the model for 10 epochs. Use batch size of 32\n",
    "norm_hist = model.fit(...)\n",
    "###End of coding segment\n",
    "\n",
    "#Let's visualise the losses and accuracies of both sessions\n",
    "print('\\n\\n' + ('-=' * 20))\n",
    "print('performance of model on non-normalized data')\n",
    "utils.plotter(nonnorm_hist)\n",
    "\n",
    "print('performance of model on normalized data')\n",
    "utils.plotter(norm_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nv4eJmgriJ3"
   },
   "source": [
    "**What do you think, is it working well?**\n",
    "\n",
    "**Can you see signs of overfitting?**\n",
    "\n",
    "If you think you observe overfitting, or think the model needs to train longer, go back and change the number of epochs to a what you think a good estimate will be.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Once you're satisfied with the training result, run the cell below to test the model on a few images.\n",
    "\n",
    "OPTIONAL: take a picture of yourself and upload to this notebook. On the expandable menu to the right you can upload the image from your computer. Put it in the folder 'test_images', then run the cell below.\n",
    "\n",
    "If you don't want to test the model on yourself, just run the cell below anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3433
    },
    "colab_type": "code",
    "id": "wisIhD1mriJ3",
    "outputId": "a4302906-b776-448d-a0ce-3a5349a6225e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = glob('test_images/*.jpg')\n",
    "testdata = utils.prepare_celeba_testdata(max_size=100, c_channels=3)\n",
    "    \n",
    "Y = model.predict(np.asarray(testdata))\n",
    "\n",
    "for prediction, img in zip(Y, images):\n",
    "    img = cv2.imread(img)\n",
    "    genders = ['Female', 'Male']\n",
    "    plt.title('I think this is a: %s' %(genders[int(round(prediction[0]))]))\n",
    "\n",
    "    plt.imshow(img[...,::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2P3fscJriJ6"
   },
   "source": [
    "**Can you explain what you've observed? Why would the behaviour be different with normalized input data?**\n",
    "\n",
    "If you want, tweak the model parameters such as epochs, loss and optimizer and observe the effects. Can you improve the learning of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdRFYoL_riJ7"
   },
   "source": [
    "### OPTIONAL: Should You Standardize Your Data?\n",
    "\n",
    "Another pre-processing step you might come across is standardizing the data. This means scaling the data so that it conforms to a gaussian distribution with mean zero and standard deviation of 1. Whether standardizing is appropriate depends on your data, specifically its distribution.\n",
    "\n",
    "**Can you think of the reason why?**\n",
    "\n",
    "-------\n",
    "\n",
    "As with many deep learning problems this is also an empirical one. Test the effects on model performance, does it improve with standardized data? Great, keep it! Note that normalization changes the numeric range without losing information (the distribution remains the same), whereas standardization changes the underlying distribution and can be sensitive to data containing outliers.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Standardise the images stored in variable *X_data*, and observe the effects of standardization on performance.\n",
    "\n",
    "**Hint:** standardization is performed by the formula:\n",
    "\n",
    "![standardization](http://www.paulvangent.com/files/DL_Course/day2_images/standardization.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1333
    },
    "colab_type": "code",
    "id": "07xK99hGriJ7",
    "outputId": "db6f2dab-7240-4525-c57c-6b4b4fe0d69d"
   },
   "outputs": [],
   "source": [
    "X_standardized = np.zeros((2000, 100, 100, 3), dtype=np.float32)\n",
    "\n",
    "for i in range(len(X_data)):\n",
    "    img = X_data[i]\n",
    "    \n",
    "    ###Start of coding segment\n",
    "    #Standardize img\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###End of coding segment###\n",
    "\n",
    "    #We then put the standardized image into the array\n",
    "    X_standardized[i] = img_standardized\n",
    "\n",
    "\n",
    "#We initialize an empty model again and compile it\n",
    "model = convnet.build_model(input_shape=(100, 100, 3), classes=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Fit the model\n",
    "std_hist = model.fit(X_standardized[:1800], Y_train,\n",
    "                      validation_data=(X_standardized[1800:], Y_val),\n",
    "                      epochs=10,\n",
    "                      shuffle=True,\n",
    "                      batch_size=32)\n",
    "\n",
    "#Let's visualise the losses and accuracies of both sessions\n",
    "print('\\n\\n' + ('-=' * 20))\n",
    "print('performance of model on normalized data')\n",
    "utils.plotter(norm_hist)\n",
    "\n",
    "print('performance of model on standardized data')\n",
    "utils.plotter(std_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAqEtoySriJ9"
   },
   "source": [
    "**What do you think, did it help?**\n",
    "\n",
    "The end-state depends a bit on weight initialization and the path the optimizer takes, but in our case the maximum achieved accuracy was better on one set, whereas the loss was lower for the other set.\n",
    "\n",
    "**Think about this. Which metric do you trust more? Why?** Ask us if you don't know, this is important!\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "jD-HDn8DriJ-"
   },
   "source": [
    "-----------------\n",
    "\n",
    "## 2.3 Optional: Data Augmentation\n",
    "\n",
    "Data augmentation, simply put, is creating extra training examples from your existing data. Consider the following examples of how you can create variations on the same image of a kitten.\n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/kitten.jpg)\n",
    "\n",
    "Run the following cell to render some variations on this image. Take notice of what we do numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "colab_type": "code",
    "id": "erAJTN0XriJ_",
    "outputId": "70444dd8-0e8f-49a9-cade-9d9cd902b305",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "download('http://www.paulvangent.com/files/DL_Course/day2_images/kitten.jpg', 'kitten.jpg')\n",
    "\n",
    "img = np.array(Image.open('kitten.jpg'))\n",
    "\n",
    "\n",
    "#Create a noise variant of the image\n",
    "noise = np.random.randint(-100, 100, size=(img.shape[0], img.shape[1], 3))\n",
    "#generate noise between 0.25 and 0.75\n",
    "noise_amount = (0.75 - 0.25) * np.random.random_sample() + 0.25\n",
    "#add noise to image\n",
    "noisy_img = np.clip(img + (noise_amount * noise), a_min=0, a_max=255)\n",
    "plt.title('added noise')\n",
    "plt.imshow(np.uint8(noisy_img))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Create a blurred variant of the image\n",
    "blurred_img = cv2.blur(img, (7,7))\n",
    "plt.title('blurred image')\n",
    "plt.imshow(np.uint8(blurred_img))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Rotate the image 45 degrees counter clockwise\n",
    "rotated_img = rotate(img, 45, reshape=False)\n",
    "plt.title('rotated image')\n",
    "plt.imshow(np.uint8(rotated_img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6U_WmnWriKB"
   },
   "source": [
    "### So why would augmentation be useful?\n",
    "\n",
    "Generally speaking data augmentation can create a more robust model. The reason is that we're increasing the variance in the dataset by transforming or distorting the images. Your model will be forced to learn robust descriptors of the data. For example, by adding a noisy, blurred and rotated version of all images to your set, your model will learn to deal with these transformations.\n",
    "\n",
    "In many situations you may not have as much training data as you'd like. Using data augmentation in these cases can help your model learn more by providing it with more samples to learn from. \n",
    "\n",
    "### How to apply it?\n",
    "You can write your own that does exactly what you want, which in some cases is preferable. However, Keras has a nice image data generator specifically for images that will do just fine in most situations. [Have a look at the Keras documentation here](https://keras.io/preprocessing/image/).\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Apply the image data generator to the *X_train_normalized* and *X_val_normalized* set in memory, and train a model. [Look at the keras documentation for hints and examples](https://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "b2-g7RQ1riKB",
    "outputId": "a1cb8ddd-51bc-4eb9-a987-c3d48372f1f8"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "###Start of coding segment###\n",
    "datagen = ImageDataGenerator(...)\n",
    "###End of coding segment###\n",
    "\n",
    "\n",
    "model = convnet.build_model(input_shape=(100, 100, 3), classes=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "###Start of coding segment###\n",
    "model.fit_generator(...)\n",
    "###End of coding segment###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "jTB-LslxriKF"
   },
   "source": [
    "### Should you apply it?\n",
    "The main consideration for augmenting your dataset should be whether it increases performance. What do you want to predict? What are the boundary conditions of the data your model will encounter 'in the wild'? \n",
    "\n",
    "Say you want to predict someone's age from pictures of faces. If all the model will ever see at runtime is pictures with faces centered and horizontally aligned, adding rotated images to your training set makes little sense. \n",
    "\n",
    "The downside is your model needs to fit a more complex function to make predictions, which means you might need a bigger model that takes more memory and more time to compute. Think about what you want to achieve with it before choosing to augment your set or not.\n",
    "\n",
    "**Too much augmentation can also cause underfitting (as you've problably observed)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "kvQu4GzEriKF"
   },
   "source": [
    "-----------\n",
    "\n",
    "## 2.4 OPTIONAL: Splitting Your Data Into Appropriate Sets\n",
    "\n",
    "Remember from yesterday that it's common practice to split your dataset into a *training set*, *validation set* and *test set*. You use your *training set* to teach the model something, and monitor for signs of overfitting and other potential trouble by using the *validation set*. Once you finish tweaking the model and training, you use the last *test* holdout set to validate that the model indeed performs as you expect. How big should each set be? There is no magic number for this, but luckily there are some common sense rules of thumb we can follow.\n",
    "\n",
    "For machine learning problems, where the datasets are usually a lot smaller, the [pareto principle](https://en.wikipedia.org/wiki/Pareto_principle) is often used, splitting the data 80/20 (train/test). For train/val/test sets a common distribution used to be 60/20/20 (train/val/test).\n",
    "\n",
    "This is reasonable, as you need sufficient examples to evaluate the model's performance. For example, a common toy dataset used in machine learning is the [Iris dataset](https://scikit-learn.org/0.16/modules/generated/sklearn.datasets.load_iris.html) that contains data on how petal shapes of three iris plants relate to their species.\n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/220px-Iris_versicolor_3.jpg)\n",
    "\n",
    "With this set, one valid distribution could be 120 samples for training and 30 samples for testing, which gives the model enough data to learn from, and enough data for us to understand the performance. Testing a model on one or two examples will never give a good performance indication. \n",
    "\n",
    "**Can you explain why?**\n",
    "\n",
    "-------------\n",
    "\n",
    "### Moving to Deep Learning\n",
    "\n",
    "With deep learning the data sets are usually **a lot** larger. \n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/bigdata.jpg)\n",
    "\n",
    "There is no need to keep to the 80/20 or 60/20/20 conventions. This will create an unnecessarily large validation and test set, and more importantly, it deprives your model of a lot of training data. As a result, it might learn less than it otherwise would.\n",
    "\n",
    "The bottom line is: set your validation and test sets to be large enough to give you enough confidence in the performance of your model.\n",
    "\n",
    "For example, given a dataset of 1.000.000 examples, even a test set of 1% will give you 10.000 examples. This might very well be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EViUQew7riKG"
   },
   "source": [
    "### Splitting smart\n",
    "\n",
    "A particularly important consideration is not how large the split in your dataset is, but *how* you make the split. Consider you want to train a model to recognize cyclists and pedestrians. The first 10.000 images in your dataset are cyclists, the second 10.000 are pedestrians. You take the first 18.000 images (90%) to train your model. 1000(5%) are used for validation, and the last 1000 for testing. After training you find out:\n",
    "\n",
    "- Performance on both the validation and test set is excellent (let's say 98.5% correct rate).\n",
    "- When you test the system in real life, it turns out to not detect cyclists very well.\n",
    "\n",
    "**What could be the problem here? Why didn't we catch this poor performance?**\n",
    "\n",
    "**How would you solve this?**\n",
    "\n",
    "---------\n",
    "\n",
    "As you might have guessed from the example given, it is important that our validation and test sets are representative of the problem we want to solve. In the given example we only tested the model on pedestrian data since the set was ordered. This caused us to miss poor performance on detecting cyclists. This is important to check: many datasets are ordered by class. To solve this we simply shuffle the data before splitting it.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Shuffle the data below. [hint](https://pynative.com/python-random-shuffle/)\n",
    "\n",
    "- Take note: you need to shuffle both the data and the labels the same way, or you'll lose the mapping between input and output data.\n",
    "\n",
    "[the manual way hint](https://stackoverflow.com/a/23289591)\n",
    "\n",
    "[the easy way hint](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "K1N5iZh1riKG",
    "outputId": "6943db49-85da-457b-d356-49498fe40d38"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#first we load the cifar10 data again\n",
    "(X_train, Y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "\n",
    "###Start of coding segment###\n",
    "#Take a look above at both ways to shuffle. Pick one and implement below.\n",
    "\n",
    "\n",
    "###End of coding segment###\n",
    "\n",
    "#let's test a few random entries in your shuffled data\n",
    "#the labels in the output should match the pictures if you did it correctly\n",
    "for i in np.random.randint(0, len(X_train), size=(5,)):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.title(utils.c10_labels[int(Y_train[i])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgT8z3FiriKK"
   },
   "source": [
    "The Keras .fit() function has a 'shuffle' flag. You can use this from now on, but now you know how to do it manually in case you need to shuffle your dataset before splitting into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise_2_Preprocessing_and_Augmentation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
