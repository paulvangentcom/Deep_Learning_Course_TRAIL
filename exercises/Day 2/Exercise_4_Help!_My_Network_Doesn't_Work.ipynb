{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWQoiEY0Xc22"
   },
   "source": [
    "# Exercise 4 - Help! My Network Doesn't Work!\n",
    "\n",
    "Once you get to applying deep learning on your own work -or just for fun-, you will run into problems. A lot of problems. These can be a cause of headaches, time-loss and stress.\n",
    "\n",
    "![John](http://www.paulvangent.com/files/DL_Course/day2_images/John.jpg)\n",
    "\n",
    "This goal of this notebook is to give you a set of debugging tools. When things don't work, where do you start looking? Hopefully it will help you side-step a lot of frustration and wasted hours.\n",
    "\n",
    "Some sections will have an exercise where we've made a mystery mistake. It will be up to you to 'debug' our mess and tell us what we need to do better!\n",
    "\n",
    "### Index\n",
    "- 4.1 - Problems with your data\n",
    "- 4.2 - Problems with fitting the network (learning)\n",
    "- 4.3 - Do you need more data?\n",
    "- 4.4 - Problems with prediction accuracy\n",
    "\n",
    "------\n",
    "\n",
    "As always, run the cell below first to import everything needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "7aV2T71jXc23",
    "outputId": "319c4c9d-85f8-4875-e91a-5e6d628c4816"
   },
   "outputs": [],
   "source": [
    "#download required datasets for this notebook (might take a bit, be patient!)\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download(url, file):\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)\n",
    "    if not os.path.isfile(file):\n",
    "        print(\"Download file... \" + file + \" ...\")\n",
    "        urlretrieve(url,file)\n",
    "        print(\"File downloaded\")\n",
    "        \n",
    "def unzip(file):\n",
    "    with ZipFile(file) as f:\n",
    "        f.extractall()\n",
    "    print('unzipped file: %s\\n' %file)\n",
    "    \n",
    "try:\n",
    "    download('http://www.paulvangent.com/files/DL_Course/misc_day2.zip', 'misc_day2.zip')\n",
    "except:\n",
    "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754608&authkey=AOJO5mcV9eCZsJ8', 'misc_day2.zip')\n",
    "unzip('misc_day2.zip')\n",
    "\n",
    "try:\n",
    "    download('http://www.paulvangent.com/files/DL_Course/catsndogs2.zip', 'catsndogs2.zip')\n",
    "except:\n",
    "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754593&authkey=AGQ0ehV6iCgTUhQ', 'catsndogs2.zip')\n",
    "unzip('catsndogs2.zip')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import utils_day2 as utils\n",
    "from misc import convnet\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GrcVT2PXc26"
   },
   "source": [
    "## 4.1 - Problems with your data\n",
    "\n",
    "![baddata](http://www.paulvangent.com/files/DL_Course/day2_images/Iseebaddata.png)\n",
    "\n",
    "The first thing you need to check when your model isn't performing as expected, is what you're feeding it. Maybe you've made a mistake in scaling your labels, maybe your labels are ordered differently from your training samples, or maybe the data is not being loaded properly. As with any modelling task: **Garbage in: Garbage out!**\n",
    "\n",
    "Rather than checking every single thing every time something goes wrong, we recommend you look at the following things:\n",
    "\n",
    "### 0. Is your input loaded?\n",
    "When loading (imoage) data, some libraries may return an array with 'NaN' or zeros whenever they run into an issue. This array could have the expected shape! Be sure to check and/or visualise the data after you load it into a training set.\n",
    "\n",
    "### 1. Did you normalize / standardize inputs?\n",
    "As you learned in Exercise 2, normalization of data and labels can have a substantial impact on training time and model efficacy, to the point of making the difference between the model learning or not learning anything.\n",
    "\n",
    "### 2. Did you scale your input labels? Do they still make sense?\n",
    "Visualise, visualise, visualise! Quite often you may accidentally have mixed data axes. For example when dealing with images, opencv and numpy use different axis orders. Always visualise or plot your data after preprocessing steps and ensure that whatever property is important is preserved. In one case this led to the following labeled data for faces:\n",
    "\n",
    "**-image of swapped landmarks-**\n",
    "\n",
    "### 3. Is your input connected to your output?\n",
    "Even if you didn't scale the labels, did you load them properly? Make sure that whatever data you feed the network matches the output label you want it to map to. The network has no conceptual knowledge of anything that's happening, so it doesn't care if you feed it an image of a cat or a few minutes of Beethoven's 9th (assuming at least the array shapes match). Again, visualise before fitting, don't jump in blindly.\n",
    "\n",
    "### 4. How noisy is your data?\n",
    "There will be bad labels in many datasets. If you collect and label data yourself, or someone else does it, mistakes will be made. Usually the network will fit just fine even with high numbers of incorrect labels assuming the errors are somewhat random. For example, [this paper](https://arxiv.org/pdf/1412.6596.pdf) still learned from MNIST with half the labels being wrong!\n",
    "\n",
    "### 5. Did you shuffle your training data?\n",
    "If most batches you feed to your network contain a single class, the network will not learn optimally. Make sure each batch contains examples of multiple classes. You can achieve this by shuffling the data before training (See 2.4)\n",
    "\n",
    "### 6. How many of each class are there in your set?\n",
    "If your task is classification, check how many examples do you have of each class? Does 80% of your datasets consist of a single class? This can be problematic. You can try augmenting the data for the under-represented class, change your loss function to adjust for the imbalance, or you can try one of [several other techniques for class imbalance](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "**Exercise:** The model below tries to classify whether the images are of a cat or a dog, but it doesn't work: it doesn't seem to learn from the training set and right off the bat gets 100% on our validation set. These are both red flags! \n",
    "\n",
    "We suspect it's because we made some mistakes in handling the data. Use the checklist above to find out what we did wrong. \n",
    "\n",
    "- Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "O0LQhh9aXc27",
    "outputId": "e04a25db-6ff0-4f9f-ed76-2144807b5ded",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = convnet.build_model(input_shape=(100,100,3), classes=2,\n",
    "                            final_activation = 'softmax')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X, Y = utils.get_data_exercise4()\n",
    "\n",
    "model.fit(X[:3800], Y[:3800], \n",
    "          epochs=5, \n",
    "          validation_data=(X[3800:], Y[3800:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwgxA7nuXc2_"
   },
   "source": [
    "**What do you see happening? What could be an issue?**\n",
    "\n",
    "- Use the cell afterwards to explore the data \n",
    "- Then describe the problem in the cell after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZW4qJqxPXc2_"
   },
   "outputs": [],
   "source": [
    "#Use this cell to explore the data\n",
    "##Start of coding segment##\n",
    "\n",
    "\n",
    "\n",
    "###End of coding segment###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R84xO1FxXc3C"
   },
   "source": [
    "**Describe your problem here** (double click this text to edit it, press shift+enter when you're done)\n",
    "\n",
    "\n",
    "I've seen the following problems:\n",
    "\n",
    "- ...\n",
    "\n",
    "\n",
    "I would solve this by:\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYGA68wOXc3C"
   },
   "source": [
    "-----------\n",
    "\n",
    "### Fixing our mess\n",
    "\n",
    "**Exercise:** Fix the problems you discovered and re-train the model so that it works better than our version.\n",
    "\n",
    "- You can implement whatever fixes you want in the cell below\n",
    "- You should be able to reach >95% validation accuracy\n",
    "\n",
    "Hint: we will tell you the problem is *with the data, not the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "0CxkM0YoXc3D",
    "outputId": "3b0da220-151f-40b1-ed3a-43379caa17a7"
   },
   "outputs": [],
   "source": [
    "model = convnet.build_model(input_shape=(100,100,3), classes=2,\n",
    "                            final_activation = 'softmax')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X, Y = utils.get_data_exercise4()\n",
    "\n",
    "\n",
    "###Start of coding segment###\n",
    "\n",
    "\n",
    "###End of coding segment###\n",
    "\n",
    "model.fit(X[:3800], Y[:3800], \n",
    "          epochs=10, \n",
    "          validation_data=(X[3800:], Y[3800:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "vFjid0M1Xc3I"
   },
   "source": [
    "This exercise was based on an excerpt from the [cats vs dogs kaggle competition](https://www.kaggle.com/c/dogs-vs-cats/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAw2Xo8qXc3J"
   },
   "source": [
    "## 4.2 - Learning Problems\n",
    "\n",
    "![learning problems](http://www.paulvangent.com/files/DL_Course/day2_images/waitingtotrain.jpg)\n",
    "\n",
    "So you've checked your data and found no issues, but the network is still not learning. What can you do now?\n",
    "\n",
    "### 1. Can the network fit the data?\n",
    "Check if your network has enough capacity to fit the function required. try overfitting on a really small subset of the data. For example take 2 images of each class, turn off any automated early stopping and let the network overfit. If training accuracy doesn't get very high, the network may not have enough capacity or there may be some software defect preventing fitting. Check how you load the data, how you build the network, and how you fit. If you cannot find anything wrong with your code, try adding layers to create a deeper network, widening existing layers, or use different architecture.\n",
    "\n",
    "### 2. Is the activation function on the output layer appropriate?\n",
    "This is something a lot have fallen into especially when re-using network architectures. Consider you've used a network for a classification task in the past, and now switch to predicting 4 coordinates for a face bounding box (like we did in Exercise 3). For the class-based approach you likely have a Softmax function on the final layer, which scales the output vector so that it has a maximum sum of 1. This is great for mutually exclusive classes, less so for predicting 4 coordinates (whose sum will likely never be exactly 1).\n",
    "\n",
    "### 3. Try less regularization\n",
    "Do you have a lot of dropout layers in your network, a lot of batchnormalization layers, or a lot of other regularizing tricks? Turn them off and run the network in a 'no bells and whistles' variant. Does it fit better now? You may have too much regularization in the network.\n",
    "\n",
    "### 4. Try a different weight initialization\n",
    "Do you initialize your weights when compiling the network? The way you initialize might by bad luck lead your optimizer to end up in a local minimum. See [this part of the Keras API for initializers](https://keras.io/initializers/). When in doubt, start with Xavier (glorot_uniform) or He initializers.\n",
    "\n",
    "### 5. Tune the hyperparameters\n",
    "- Check your learning rate, change it a little to see if it helps the network fit.\n",
    "    - some optimizers have multiple parameters to set, but start with the learning rate.\n",
    "- Try a different batch size, it may be too large or small.\n",
    "\n",
    "If you have enough compute time available, you can try a grid search. This means systematically (and automatically) trying different hyperparameter combinations and noting down the effects on network performance. [see the link here for an example on how to implement](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/).\n",
    "\n",
    "### 6. Try a different optimizer\n",
    "The optimizer you're using may not be able to converge to a minimum. Try a different optimizer to see if it has an effect.\n",
    "\n",
    "### 7. Give it more time\n",
    "Sometimes you need to let the network run for multiple epochs before it will start learning. Giving it a few epochs more time to start fitting might help.\n",
    "\n",
    "### 8. Is the problem solvable at all?\n",
    "Try [one of the standard networks baked into Keras](https://keras.io/applications/). Do these networks reach acceptable performance levels? If not, consider that the problem may not be easily solvable in its current form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "6rdf6l7uXc3K"
   },
   "source": [
    "----------\n",
    "\n",
    "**No exercise!**\n",
    "\n",
    "This section has no exercise. We hope that whenever you run into issues with your models, you will return here for troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1mTlCv5Xc3L"
   },
   "source": [
    "# 4.3 - Do you need more data?\n",
    "\n",
    "![more data](http://www.paulvangent.com/files/DL_Course/day2_images/moredata.jpg)\n",
    "\n",
    "if performance on the training set is poor, the model is not using what is in the set properly. Throwing more data at it will not change this. Make sure there are no quality problems with the data as discussed under **4.1**.\n",
    "\n",
    "However, if the model performs well on the training set but not on the test set, gathering more data is usually very effective. The model can learn, but doesn't have enough examples to learn to generalize. You can also try augmentation on the data you have to see whether that improves accuracy.\n",
    "\n",
    "**Additional remark: consider what is the quality of your data set**\n",
    "Look critically at what you have. Maybe you can clean the dataset by reducing noise in your samples or correcting incorrect labels. See what is discussed under **1**\n",
    "\n",
    "------\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "Consider the training log output below.\n",
    "\n",
    "|epoch | acc | loss | val_acc | val_loss|\n",
    "|------|-----|------|---------|---------|\n",
    "|0|0.6668|0.6502|0.5|0.7121\n",
    "|1|0.6692|0.6391|0.5|0.7405\n",
    "|2|0.6692|0.6307|0.5|0.7287\n",
    "|3|0.666|0.6277|0.5|0.7646\n",
    "|4|0.6728|0.6171|0.56|0.7096\n",
    "|5|0.682|0.5963|0.51|0.7655\n",
    "|6|0.72|0.5425|0.555|0.7345\n",
    "|7|0.7844|0.4577|0.605|0.7682\n",
    "|8|0.8452|0.3566|0.605|0.9756\n",
    "|9|0.906|0.2335|0.6|1.4574\n",
    "|10|0.9344|0.1659|0.625|1.2655\n",
    "|11|0.9604|0.1031|0.59|1.4402\n",
    "|12|0.9692|0.0851|0.585|1.6788\n",
    "|13|0.9812|0.0559|0.565|2.0274\n",
    "|14|0.9784|0.0599|0.595|2.0819\n",
    "|15|0.9956|0.0173|0.63|2.7099\n",
    "|16|0.9892|0.0322|0.575|2.4551\n",
    "|17|0.9924|0.0234|0.59|2.8146\n",
    "|18|0.9892|0.0267|0.61|2.8822\n",
    "|19|0.9836|0.0441|0.615|2.1279\n",
    "\n",
    "**Would more data help? Why (not)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgBMsJs3Xc3M"
   },
   "source": [
    "# 4.4 - Problems with predicting\n",
    "\n",
    "![img](http://www.paulvangent.com/files/DL_Course/day2_images/mistakes.jpg)\n",
    "\n",
    "What if your model worked well in training, performance on both the validation and test set was great as well, but when predicting on other data performance is disappointing?\n",
    "\n",
    "The most common cause of this is that the data you're feeding during prediction differs significantly from what you trained the network on. If, for example, you trained your network to detect cats in urban environments, don't be surprised when it doesn't work well when detecting cats in forest environments! Deep learning networks are really great at learning exactly what you tell them.\n",
    "\n",
    "This happens more often than you think as you cannot always predict exactly what 'real life data' will look like. [Take a look at this paper about generalization issues of deep learning in hospital settings for example](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "YZVyEEKGXc3M"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "[This blog is a more in-depth version of this notebook](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kF4NOYlXc3N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise_4_Help!_My_Network_Doesn't_Work.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
