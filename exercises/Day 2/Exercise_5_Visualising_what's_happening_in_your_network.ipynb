{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 5 - Visualising what's happening in your network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p51ID4NxgAeq",
        "colab_type": "text"
      },
      "source": [
        "# 6 - Visualising what's happening in your network\n",
        "\n",
        "Deep learning has often been marked as a black box device. This is used as criticism, if we don't know what's going on inside, how can we trust the results? Others say that as long as you properly validate the methods, they can be incredibly powerful. A large body of successes backs up the claim.\n",
        "\n",
        "No matter how you feel on this topic, recently tools have appeared that allow you to look into the deep network and visualise how layers encode the information they are learning.\n",
        "\n",
        "This notebook is mainly for fun, there will be no code exercises\n",
        "\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HFlYzWrgAer",
        "colab_type": "text"
      },
      "source": [
        "### A little bit of this, and a little bit of that\n",
        "\n",
        "You might recall from the lecture that a convnet builds complex representations of an object by assembling smaller building blocks. An intuitive visualisation of filters in the convnet is displayed in the following image:\n",
        "\n",
        "![img](http://www.paulvangent.com/files/DL_Course/day2_images/facelayers.png)\n",
        "\n",
        "You can clearly see how simple line detectors can be used to combine into 'building blocks' like noses, eyes, and mouths that are then assembled into representations of a face!\n",
        "\n",
        "-----\n",
        "\n",
        "### Playing around yourself\n",
        "\n",
        "An interactive visualisation of how an input is propagated through a convnet is made by Adam Hayley. [We recommend playing around a bit with the tool to observe what happens!](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) You can move your mouse over any pixel in the layers to see how that pixel is computed.\n",
        "\n",
        "--------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKQiXK77gAer",
        "colab_type": "text"
      },
      "source": [
        "### Recognizing birds\n",
        "\n",
        "A more complex visualisation is displayed on the github page of [Keras-Vis](https://github.com/raghakot/keras-vis) based on a few examples from their github. Keras-vis is a tool that can help you visualise network layers and see what's going on. This can help debugging, but it also just very cool too look at.\n",
        "\n",
        "As an example the encoding of an Ouzel (type of bird) in the network is given:\n",
        "\n",
        "![ouzel](http://www.paulvangent.com/files/DL_Course/day2_images/ouzel1.png)\n",
        "\n",
        "The interesting thing to note is that the bird is encoded in various rotations, showing how the network has learned to cope with recognizing it in different orientations!\n",
        "\n",
        "------\n",
        "\n",
        "### Different categories\n",
        "\n",
        "Encodings for objects from different categories visualised by Keras-Vis might look like this.\n",
        "\n",
        "Some make sense if you look a little longer, others may be unrecognizable to us humans.\n",
        "\n",
        "![img](http://www.paulvangent.com/files/DL_Course/day2_images/kerasvis.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJK9YN15gAes",
        "colab_type": "text"
      },
      "source": [
        "### What are you looking at?\n",
        "\n",
        "One final interesting thing to note is [this paper](https://arxiv.org/pdf/1610.02391v1.pdf) which is also mentioned on the Keras-Vis repository.\n",
        "\n",
        "The paper details a methodology to visualise exactly where a network puts most of its 'attention' when making a decision. This gives information on what features the network uses to make a prediction:\n",
        "\n",
        "![attention](http://www.paulvangent.com/files/DL_Course/day2_images/attention.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAO_yFfvgAet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}