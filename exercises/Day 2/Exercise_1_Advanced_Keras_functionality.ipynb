{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 1: Advanced Keras Functionality\n",
    "\n",
    "In this notebook we will show you some of Keras' more advanced features.\n",
    "\n",
    "### Index:\n",
    "1. Loading a trained model\n",
    "2. Resuming training on a previously saved model\n",
    "3. Saving an updated model\n",
    "4. Using a data generator with very large datasets\n",
    "\n",
    "\n",
    "First of all let's import the requirements for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download file... misc_day2.zip ...\n",
      "File downloaded\n",
      "unzipped file: misc_day2.zip\n",
      "\n",
      "Download file... celebfaces.zip ...\n"
     ]
    }
   ],
   "source": [
    "#download required datasets for this notebook (might take a bit, be patient!)\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download(url, file):\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)\n",
    "    if not os.path.isfile(file):\n",
    "        print(\"Download file... \" + file + \" ...\")\n",
    "        urlretrieve(url,file)\n",
    "        print(\"File downloaded\")\n",
    "        \n",
    "def unzip(file):\n",
    "    with ZipFile(file) as f:\n",
    "        f.extractall()\n",
    "    print('unzipped file: %s\\n' %file)\n",
    "    \n",
    "try:\n",
    "    download('http://www.paulvangent.com/files/DL_Course/misc_day2.zip', 'misc_day2.zip')\n",
    "except:\n",
    "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754608&authkey=AOJO5mcV9eCZsJ8', 'misc_day2.zip')\n",
    "unzip('misc_day2.zip')\n",
    "\n",
    "download('http://www.paulvangent.com/files/DL_Course/day2_images/artificialintelligence.jpg', 'artificialintelligence.jpg')\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from glob import glob\n",
    "from keras import backend as K\n",
    "from keras import Sequential, callbacks\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "from utils_day2 import mark_face\n",
    "\n",
    "#Define custom loss function\n",
    "def euclidean_distance(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))\n",
    "\n",
    "keras.losses.euc_dist_keras = euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a trained model\n",
    "\n",
    "In the previous two notebooks you learned about training a deep learning model. This training resulted in a model that fulfilled a task, such as recognising a handwritten digit, or recognising one of 10 different object classes in pictures.\n",
    "\n",
    "You might then want to save the model so that you can re-use it later. A situation might also occur where you want to stop training now and resume at a later time, for example when you're renting computing power or need your desktop computer for another task. To accomplish this you would need to save the model to your hard drive so you may retrieve it later.\n",
    "\n",
    "Keras comes with handy save and load functionality built-in, so that's a spot of luck!\n",
    "\n",
    "***\n",
    "\n",
    "For this exercise we have provided you with a model we've trained on the CelebFaces dataset using a GPU. It is capable of predicting where the face in an image is, but we only trained it for a few epochs, so it doesn't work optimally yet. You will learn how to load the model and make a prediction, update the model by training for one epoch on a small additional dataset, and saving it again.\n",
    "\n",
    "To do so, first load the model using the **load_model()** function\n",
    "- the path to the model is 'celebfaces/celebfaces_model.hdf5'.\n",
    "- we are using a custom loss function, so set compile=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Add your code here (approx 1 line)\n",
    "#load the model, set compile=False\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts coordinates (x,y) of where the face on an image is, so we use the euclidean distance as a loss function. The formula looks like this:\n",
    "\n",
    "![euclidean loss](http://www.paulvangent.com/files/DL_Course/day2_images/eucloss.jpg)\n",
    "\n",
    "The closer the predicted coordinates are to the real ones, the lower the loss. We need to define the loss function using keras (tensorflow) operations. This has to do with where the loss needs to be computed (especially if we're using a gpu, we cannot just execute arbitrary code on it).\n",
    "\n",
    "**Take a look at our definition of the euclidean loss below. Can you explain how it works?**\n",
    "\n",
    "**Note:** Don't forget to run the cell below after you've taken a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define custom loss function\n",
    "def euclidean_distance(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing to do is to compile the model. You've done this a few times by now so we won't give you a hint. If you get stuck, take a look at an earlier notebook.\n",
    "\n",
    "Use our custom loss function by assigning the loss to the function to the function we've defined (**\"loss=euclidean_distance\"**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Add your code here (approx 1 line)\n",
    "#compile the model with the custom loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first make a prediction to see what the model does.\n",
    "\n",
    "We have provided you with a function **mark_face()**. It takes two arguments:\n",
    "- the image 'img'\n",
    "- the predicted coordinates 'prediction'\n",
    "\n",
    "Run the cell below to plot predictions for the first five images in the dataset. Don't forget to take a look at the code to understand what happens. You can ask us if you're unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    img_normalised = img / 255. #standardise image\n",
    "    prediction = model.predict(img_normalised.reshape(1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    return prediction\n",
    "\n",
    "def plot_result(img, prediction):\n",
    "    #we need to reverse the RGB channels, otherwise 'matplotlib.pyplot' gets confused\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #mark the face\n",
    "    img = mark_face(img, prediction[0])\n",
    "    #show the result\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "images = sorted(glob('celebfaces/images/*.jpg'))\n",
    "for image in images[0:5]:\n",
    "    img = cv2.imread(image)\n",
    "    prediction = predict(img)\n",
    "    plot_result(img, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works quite well already, but in the third and the last image *the face isn't marked completely*. We will update the model on these images and re-run the prediction so you can see the model indeed updates.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resuming training on a previously saved model\n",
    "\n",
    "Training deep learning models can take a lot of time, especially with larger architectures and/or on larger datasets. You might need your computer for something else in between for example. You can easily interrupt training and resume later. In this exercise you will learn how by updating the model we've loaded in the previous section.\n",
    "\n",
    "***\n",
    "\n",
    "In the previous section you loaded the celebfaces model. It will now behave just like any normal Keras model, and you've learned to deal with these in the previous notebooks already! This means you already know how to update the model with new data.\n",
    "\n",
    "Let's update the model now by running a short training session on 50 images. We will limit the training phase to save you time, since the exercise is only about illustrating the process.\n",
    "\n",
    "First run the cell below to load the images and labels into memory and normalise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(images):\n",
    "    output = []\n",
    "    for image in images:\n",
    "        im = cv2.imread(image)\n",
    "        im = im / 255. #normalise image pixel values\n",
    "        output.append(im)\n",
    "    return np.asarray(output)\n",
    "\n",
    "images = glob('celebfaces/images/*.jpg')\n",
    "X = load_images(images)\n",
    "Y = np.memmap('celebfaces/Y.npy', dtype=np.float32, mode='r', shape=(50, 4))\n",
    "Y = Y / 250. #original images + labels are 250x250px, normalise labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your exercise is now to fit the model. Once you run the cell it might take a few minutes to complete. Use the following settings for the fitting:\n",
    "\n",
    "- X, Y for training data, training labels\n",
    "- 4 epochs is enough for now\n",
    "- use a batch size of 8\n",
    "\n",
    "[hint](https://keras.io/models/model/#fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Add your code here (approx 1 line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should've seen the model go through training and the weights changing. It's ok if the accuracy drops (it might, it might not): we are using a very small training set so the model likely has some issues fitting the new data in with so short a training window and so few examples (50). The overall loss should decrease nonetheless as the weights update to better accomodate the new data.\n",
    "\n",
    "Run the cell below to re-evaluate performance on the third and fifth image on which it didn't do too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs = ['celebfaces/images/000003.jpg', 'celebfaces/images/000005.jpg']\n",
    "\n",
    "for im in imgs:\n",
    "    img = cv2.imread(im)\n",
    "    prediction = predict(img)\n",
    "    plot_result(img, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! The prediction has indeed changed for the better, showing our model weights have been updated.\n",
    "\n",
    "### However, think about this:\n",
    "We have predicted on an image that we also used to update the model weights (train it) on. We've done this to show you the weights do indeed change without requiring you to wait through long training sessions. **Can you explain why this doesn't necessarily mean our model has improved? Why would you (not) use this approach to update your own models?** If you don't know, ask us. This is a really important point. This is also the mistake we made.\n",
    "\n",
    "***\n",
    "\n",
    "We used a few memes in the previous notebook, of course you could opt to run the model on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('artificialintelligence.jpg')\n",
    "prediction = predict(img)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = mark_face(img, prediction[0], dx=img.shape[1], dy=img.shape[0])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving an updated model\n",
    "\n",
    "Saving a model in Keras is very easy. The model class has its own 'save' method. You can simply call this and specify a filename. Use the 'hdf5' extension as this is the data format used by Keras models. \n",
    "\n",
    "Your task now is to save the model.\n",
    "\n",
    "- Use the .save() functionality.\n",
    "- use filename: 'celebfaces/newmodel.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Add your code here (approx 1 line)\n",
    "\n",
    "##End of your code\n",
    "\n",
    "print(glob('celebfaces/*.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see \"my_awesome_model.hdf5\" appear in the detected files. You can find the model in the 'celebfaces' subfolder of this notebook.\n",
    "\n",
    "\n",
    "### Moving away from Keras\n",
    "There may be situations where you need to run your models on a different platform. You could begin again from scratch on the new platform, but usually you don't need to. Remember the model consists of an architecture as well as the weights, and it's the weights that are trained.\n",
    "\n",
    "Using the .save() function you actually save both the model architecture and its weights, as well as things like optimizer state. **You can also save them separately**, which makes migrating to a different deep learning toolkit easier. Usually these toolkits have ways to cross-import model weights.\n",
    "\n",
    "- To do so, simply use the **.save_weights()** function. \n",
    "- You can also save the model architecture separately to a JSON file for easy interchange.\n",
    "\n",
    "We demonstrate both methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model weights\n",
    "model.save_weights('celebfaces/my_awesome_weights.hdf5')\n",
    "\n",
    "#save model architecture to JSON\n",
    "model_json = model.to_json() #creates a JSON memory object\n",
    "with open(\"celebfaces/awesome_model.json\", \"w\") as f: #we open a file object\n",
    "    f.write(model_json) #we stream the JSON memory object to the file\n",
    "    \n",
    "print(glob('celebfaces/*.hdf5'))\n",
    "print(glob('celebfaces/*.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using a data generator with very large datasets\n",
    "\n",
    "Once you graduate from using smaller, 'toy' datasets into real-world deep learning problems, you run into massive datasets. For example, UC Berkeley recently open sourced a large dataset for training deep learning networks for use in traffic. The set contains **100,000 videos and 120,000,000 images!** Sizes like that are not uncommon.\n",
    "\n",
    "In the examples so far we've simply loaded all image data and labels into memory and used that. We cannot do this with large datasets. For example, assuming a set of 'only' 10,000 colour images of 720p resolution, we would need: (1280 \\* 720 \\* 3 \\* 32) \\* 10,000 = 884,736,000,000 bits, or approximately **110 gigabytes of memory**, simply to store them in a numpy array in memory. Your average laptop has 8 to 16 gigabytes of memory. Now imagine what you would need for the full set of 120,000,000 images... This is simply impossible.\n",
    "\n",
    "***\n",
    "\n",
    "The solution is simple: we only load the images the model needs, and only when it needs them. In other words: we generate each batch on the fly.\n",
    "\n",
    "Keras can work with so-called 'data generators' that do exactly this.\n",
    "\n",
    "**Note** that this example requires some more advanced understanding of python. For this reason, and because data generators are super useful, we will give you a basic example and explain what it does with comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filelist):\n",
    "    X = []\n",
    "    for f in files:\n",
    "        img = cv2.imread(f)\n",
    "        X.append(X)\n",
    "    return np.asarray(X)\n",
    "\n",
    "def myGenerator(batch_size=10):\n",
    "    #Instead of loading images, we load the paths to the images.\n",
    "    #This makes sure we don't use a lot of memory, even for massive datasets!\n",
    "    # As a working example we load the 50 celebface images\n",
    "    X_files = glob('celebfaces/images/*.jpg')\n",
    "    #count how many files we have\n",
    "    num_files = len(X_files)\n",
    "    \n",
    "    #We do load all the labels into memory, since these do not use a lot of memory anyway\n",
    "    Y_labels = np.memmap('celebfaces/Y.npy', dtype=np.float32, mode='r', shape=(50, 4))\n",
    "    #don't forget to check if labels also need standardisation, in this set they do!\n",
    "    Y_labels = Y_labels / 250.\n",
    "    \n",
    "    #calculate the number of epochs given the training size and batch size\n",
    "    num_epochs = num_files / epochs\n",
    "    \n",
    "    #Data generators need to be able to run infinitely\n",
    "    #we simply use an always True statment\n",
    "    while True: \n",
    "        #loop over each epoch\n",
    "        for i in range(num_epochs):\n",
    "            #load the data (you would define your own function to do so)\n",
    "            files = X_files[i * batch_size : (i + 1) * batch_size]\n",
    "            #load the files into array X with function load_data()\n",
    "            X = load_data(files)\n",
    "            #retrieve corresponding labels\n",
    "            Y = Y_labels[i * batch_size : (i + 1) * batch_size]\n",
    "            #yield is the same as 'return', except 'return' terminates the function execution whereas 'yield' does not\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
