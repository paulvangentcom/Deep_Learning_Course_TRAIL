{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 2 - Convolutional Neural Nets.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e2jY_ujjBE",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 2 - Convolutional Neural Nets (CNN's)\n",
        "\n",
        "In the last notebook you built a feedforward net, and found that it could recognise handwritten digits remarkably well. However, when trying to recognise diverse objects on images we ran into performance issues. In this notebook you will learn to implement a simple CNN in Keras and apply it to CIFAR-10.\n",
        "\n",
        "The set-up of this notebook is similar to the previous. You are expected to complete certain parts of the code to be able to run it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5JBgl8LjjBF",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages\n",
        "\n",
        "First we begin by importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Q--I6jjjBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ef42dbaf-d1a1-4320-b7bd-8cd65547e597"
      },
      "source": [
        "#download required datasets for this notebook (might take a bit, be patient!)\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if os.path.isfile(file):\n",
        "        os.remove(file)\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "        \n",
        "def unzip(file):\n",
        "    with ZipFile(file) as f:\n",
        "        f.extractall()\n",
        "    print('unzipped file: %s\\n' %file)\n",
        "\n",
        "try:\n",
        "    download('http://www.paulvangent.com/files/DL_Course/utils_day1.py', 'utils_day1.py')\n",
        "except:\n",
        "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754588&authkey=ANqFqQOFr4Iqy1A', 'utils_day1.py')\n",
        "\n",
        "import numpy as np #import numpy package\n",
        "import matplotlib.pyplot as plt #import plotting library\n",
        "import utils_day1 as utils #import helper functions\n",
        "\n",
        "#import required keras modules\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, Conv1D, MaxPooling2D, RepeatVector"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download file... utils_day1.py ...\n",
            "File downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Z4JQBVjjBJ",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Dataset\n",
        "\n",
        "We load the CIFAR-10 dataset again. \n",
        "\n",
        "**Note** that we do not need to flatten the images. Remember from the lecture that a convolution can operate on two dimensional image data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si1CoMHxjjBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(X_train, Y_train), (X_val, Y_val) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33ZoVQsljjBL",
        "colab_type": "text"
      },
      "source": [
        "We do, however, still need to apply one-hot encoding and normalise the images. In the last notebook you did this with [numpy.eye()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html).\n",
        "\n",
        "Your task here is to use a built-in keras function, called [to_categorical](https://keras.io/utils/#to_categorical). Complete the code below to convert Y_train and Y_val to one-hot encoding using this function. Look at the link for a hint, and note that the dataset has **10 classes**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skZhkR_SjjBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "#Apply one-hot encoding\n",
        "##Start your code here (approx. 2 lines)\n",
        "Y_train = None\n",
        "Y_val = None\n",
        "##End of your code\n",
        "\n",
        "#Normalise the images\n",
        "##Start your code here (approx. 2 lines)\n",
        "X_train = None\n",
        "X_val = None\n",
        "#End your code here\n",
        "\n",
        "print('Shape of first training label: ' + str(Y_train[0].shape))\n",
        "print('Shape of last validation label: ' + str(Y_val[-1].shape))\n",
        "print('mean pixel value of first training example: ' + str(np.mean(X_train[0])))\n",
        "print('mean pixel value of last validation example: ' + str(np.mean(X_val[-1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbbjHfywjjBO",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "**Expected output:**\n",
        "\n",
        "Shape of first training label: (10,)  \n",
        "Shape of last validation label: (10,)  \n",
        "mean pixel value of first training example: 0.405675551471  \n",
        "mean pixel value of last validation example: 0.423383884804"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCkZUmaWjjBO",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Defining a CNN in Keras\n",
        "\n",
        "Implementing a CNN in Keras is very similar to implementing a feedforward network, with the exception that we use different layer types. For an overview of all available convolutional layers, see [this link](https://keras.io/layers/convolutional/).\n",
        "\n",
        "[**Please take a look here for some additional reading material regarding Convolutions**](https://arxiv.org/pdf/1603.07285v1.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rSZFiCUjjBP",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "Your task is to implement a CNN with the following structure:\n",
        "\n",
        "- Convolutional 2D input layer. Use the following parameters:\n",
        "    * 32 filters\n",
        "    * kernel size of (3, 3)\n",
        "    * input shape equal to input_shape\n",
        "    * relu activation\n",
        "- Max pooling 2D layer. Use a poolsize of (2, 2)\n",
        "- Regularisation layer. Use a dropout rate of 20%\n",
        "- Flattening layer \n",
        "- Dense layer, use 256 units and relu activation.\n",
        "- Output layer with 10 classes and softmax activation.\n",
        "\n",
        "\n",
        "Can you explain **why** we need to add a flatten layer between the CNN part of the network and the Dense layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNximxifjjBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "    model = Sequential()\n",
        "    ##Add your code here (approx. 6 lines)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ##End of your code\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq_0LLgMjjBR",
        "colab_type": "text"
      },
      "source": [
        "We then initialise the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsRpn-94jjBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = baseline_model()\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oA1QYLyjjBT",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "**Expected Output:**\n",
        "\n",
        "![model summary](http://www.paulvangent.com/files/DL_Course/day1_images/cnn_network.jpg)\n",
        "\n",
        "**Compare this to the FeedForward network summary from the previous notebook (we've recreated the image below).**\n",
        "\n",
        "![FFN model](http://www.paulvangent.com/files/DL_Course/day1_images/feedforwardmodel.jpg)\n",
        "\n",
        "**Can you explain why the CNN has fewer parameters, yet gets much better results?**\n",
        "\n",
        "\n",
        "***\n",
        "## 4 - Loss Functions and Optimization\n",
        "\n",
        "This section will detail the loss function and optimization of your network.\n",
        "\n",
        "### 4.1 Loss Function\n",
        "Remember from the lecture this morning, that in order to optimize the model we need to express its accuracy. Expressing the mistakes the model makes is done with a loss function, which quantifies the distance between the expected output and the predicted output. An easy example to understand is that of the logistic loss function:\n",
        "\n",
        "![logloss](http://www.paulvangent.com/files/DL_Course/day1_images/logloss.jpg)\n",
        "\n",
        "**Note:** **y** is often used for the true label, and y-hat (**ŷ**) for the predicted value.\n",
        "\n",
        "To see what it does, note that the function has two sides (left and right of ‘+’). The function expresses the similarity of prediction and true value. It 'punishes' larger deviations more severly. When the prediction and true value are very similar the loss is low, when they differ a lot the loss is high. To think visually, consider the plot below:\n",
        "\n",
        "![loglossgraph](http://www.paulvangent.com/files/DL_Course/day1_images/log_loss_graph.png)\n",
        "\n",
        "Still not 100% clear on what happens? Consider two opposite cases:\n",
        "\n",
        "***\n",
        "<center>**y=1**  \n",
        "The right side of the formula falls off because (1 – y) = 0, so that implies that for -log(ŷ) you want ŷ to be 1 (or close to 1), since log(1)=0.  </center>\n",
        "  \n",
        "<center>**y=0**  \n",
        "Now the left side of the formula falls off (0 log (ŷ) = ..). Now we need log(1 – ŷ) to be close to 0 to mach y, which happens if ŷ=0.</center>\n",
        "\n",
        "***\n",
        "\n",
        "Logloss is only applicable when there are two possible outcomes (binary classification). In the exercise below we will be using **categorical crossentropy**, which is extremely similar to logloss, except that it can handle more than two possible outcomes.\n",
        "\n",
        "[**You can see the mathematical definition of the categorical crossentropy here**](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
        "\n",
        "***\n",
        "\n",
        "### 4.2 - Optimizer\n",
        "The loss function allows us to quantify the mistakes made by model, which makes it possible to reduce them. Remember from the lectures that the optimizer will try to gradually step towards a point of minimum loss. Consider a hypothetical, one-dimensional loss function to illustrate:\n",
        "\n",
        "![onedimloss](http://www.paulvangent.com/files/DL_Course/day1_images/gradientdescent0.jpg)\n",
        "\n",
        "Here we want to minimise **C(W)**, because a low loss means the difference between predictions and actual values is small. In the context of the figure: the optimizer decides which is 'downhill', and updates the model's weights so that the loss decreases:\n",
        "\n",
        "![stepping to minimum](http://www.paulvangent.com/files/DL_Course/day1_images/gradientdescent2.jpg)\n",
        "\n",
        "An excellent overview of the different optimizers [can be found here](http://ruder.io/optimizing-gradient-descent/).\n",
        "\n",
        "In this notebook we use the Adam optimiser. In many cases Adam will converge quickly, which is useful here because many of you are running the code without a GPU.\n",
        "\n",
        "***\n",
        "**Exercise:**  \n",
        "We still need to compile the model, meaning we will be asking Keras to transform our model instructions into machine-readable code. Compile the model with a 'categorical_crossentropy' loss and an 'adam' optimizer. \n",
        "\n",
        "Hint: refer to how we did this in the previous notebook if you're unsure how to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_9ftECajjBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Add your code here (approx. 1 line)\n",
        "\n",
        "##End of your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--3OxdhjjBW",
        "colab_type": "text"
      },
      "source": [
        "Your task is now to fit the model for 10 epochs with a batch size of 256. Remember to also set the validation data sets!\n",
        "\n",
        "Hint: Refer to the previous notebook on how to do so if you are unsure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6vr5ndjjBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Add your code here (Approx. 1 line)\n",
        "history = None\n",
        "##End of your code."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWuRY0OejjBZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "***\n",
        "Note that we added 'history = ' before the model.fit command. Remember that by default, a Keras model will return a history object containing information on the training epochs. Run the cell below to plot the results of your training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_4gGjHAjjBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils.plotter(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRbwus8CjjBc",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the plot. **What do you think? Is the model overfitting or not? Why (not)?** \n",
        "\n",
        "Based on this graph, would you try to run the model for more epochs or not?\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "3m8pfcnajjBd",
        "colab_type": "text"
      },
      "source": [
        "## 5 - Expanding the CNN (optional)\n",
        "\n",
        "In this last section of the notebook we will be expanding the CNN network. A bigger network will have more capacity to fit the variance in the dataset.\n",
        "\n",
        "This section is optional because without a discrete GPU (which most of your laptops will not have), training will take long. Once it works you can turn on the model at night and let the laptop run for a while."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNCjgmIzjjBd",
        "colab_type": "text"
      },
      "source": [
        "We have copied an example of the baseline model below. Your task is to add two Conv2D layers to it.\n",
        "\n",
        "- Convolutional 2D input layer. Use the following parameters:\n",
        "    * 64 filters\n",
        "    * filtersize of (5, 5)\n",
        "    * use 'same' padding\n",
        "    * input shape of (32, 32, 3)\n",
        "    * relu activation\n",
        "- Max Pooling layer with a poolsize of 2\n",
        "- Convolutional 2D input layer. Use the following parameters:\n",
        "    * 128 filters\n",
        "    * filtersize of (3, 3)\n",
        "    * use 'same' padding\n",
        "    * input shape of (32, 32, 3)\n",
        "    * relu activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr-H1d-vjjBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expanded_model(input_shape=(32, 32, 3), classes=10):\n",
        "    model = Sequential()\n",
        "    ##Add your code here (approx. 4 lines)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ##End of your code\n",
        "    model.add(Dropout(rate=0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(rate=0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "    ##End of your code\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C_3SOvBjjBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = expanded_model()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZBjW1I9jjBh",
        "colab_type": "text"
      },
      "source": [
        "Your task here is to again fit the model. Use the same setting as earlier (10 epochs, batchsize 256)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJdXpfMwjjBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Add your code here\n",
        "history = None\n",
        "##End of your code.\n",
        "\n",
        "utils.plotter(history) #plot results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B_00tWvjjBj",
        "colab_type": "text"
      },
      "source": [
        "So, how did the model perform? Our iteration reached ~73% in the end. **Would you try extra epochs? Why (not)?**\n",
        "\n",
        "You might wonder, how does the model work on the MNIST dataset from the first notebook? It would take too much time to let the model converge on your laptop in the few hours we have today. We ran it for 25 epochs on a gpu though, and it reached **98.94%** validation accuracy. That's pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlmGi1NjjBj",
        "colab_type": "text"
      },
      "source": [
        "## 6 - Wrapping up\n",
        "\n",
        "You've reached the end of the second notebook. Hoo-ray, you're now a certified Deep Learner! You might now want to go online, get some bigger datasets, and try larger networks for yourself. You might also want to try applying what you learned to your own data.\n",
        "\n",
        "Be warned that beyond this point a GPU becomes mandatory. Even a low-budget one can speed up training dramatically, making the difference between fitting a model for a night or over a month.\n",
        "\n",
        "![GPU MANDATORY](http://www.paulvangent.com/files/DL_Course/day1_images/needgpu.jpg)\n",
        "\n",
        "***\n",
        "\n",
        "There is one last optional notebook available. This will deal with more advanced Keras features, such as saving your model for re-use later.\n",
        "\n",
        "[Go to it now!](Exercise 3 - Advanced Keras functionality.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCX5OxrEjjBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbVQHr9sjjBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}