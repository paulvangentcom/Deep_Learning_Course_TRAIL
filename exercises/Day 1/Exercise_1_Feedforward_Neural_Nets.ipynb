{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 1 - Feedforward Neural Nets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laP0zYUdi1yy",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1 - Feedforward NN\n",
        "\n",
        "In this notebook you will learn to implement a simple feedforward neural net in Keras. Your initial network will learn to recognise handwritten digits in no time! You will also learn to create a larger feedforward net and add regularisation layers to it, so that you can prevent overfitting as discussed in the lecture.\n",
        "\n",
        "This first notebook serves as a gentle introduction. At various points we will ask you to complete the given code. They will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwmJtmfbi1yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def some_function(x):\n",
        "    '''Function that squares the input, then returns the result'''\n",
        "    ##Insert your code below (approx. 1 line)\n",
        "    x = None\n",
        "    ##End of your code\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dBuB99Ui1y2",
        "colab_type": "text"
      },
      "source": [
        "Your task in these sections is the complete the code between the two lines beginning with \"##\". The number of lines we expect you to write is also given as a hint. These are approximations depending on how you choose to  solve the problem.\n",
        "\n",
        "In this case you would change the line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1iinWkNi1y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rzPWNR-i1y5",
        "colab_type": "text"
      },
      "source": [
        "into one of the options described below. Note that usually there is more than one correct solution. Where applicable, the **expected output** is given, so that you can check your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCCJyLeyi1y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x**x\n",
        "\n",
        "x = np.square(x)\n",
        "\n",
        "x = np.power(x, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-NThWeqi1y7",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages\n",
        "\n",
        "The first thing to do is import the required packages for this tutorial:\n",
        "\n",
        "- [NumPy](http://www.numpy.org) is a package optimised for (scientific) numerical computing with Python;\n",
        "- [matplotlib](https://matplotlib.org/) is a package to plot and visualise data using Python;\n",
        "- [Keras](https://www.keras.io) is the deep learning toolkit we will be using.\n",
        "\n",
        "**Note:** Please run all the cells containing code you encounter. To do so, select them, then press shift+enter, or click \"Run\" in the toolbar at the top of this page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpemBu8i1y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download required datasets for this notebook (might take a bit, be patient!)\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if os.path.isfile(file):\n",
        "        os.remove(file)\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "        \n",
        "def unzip(file):\n",
        "    with ZipFile(file) as f:\n",
        "        f.extractall()\n",
        "    print('unzipped file: %s\\n' %file)\n",
        "    \n",
        "try:\n",
        "    download('http://www.paulvangent.com/files/DL_Course/MNIST_data.zip', 'MNIST_data.zip')\n",
        "except:\n",
        "    download('https://onedrive.live.com/download?cid=39383A5AFCD95065&resid=39383A5AFCD95065%21754579&authkey=AHLvzp-6a1009g4', 'MNIST_data.zip')\n",
        "unzip('MNIST_data.zip')\n",
        "\n",
        "import numpy as np #import numpy package\n",
        "import matplotlib.pyplot as plt #import plotting library\n",
        "\n",
        "#import required keras modules\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCS7AmR1i1y-",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Dataset\n",
        "\n",
        "Now it's time to import the dataset we will be using. We've prepared a dataset for you. We will be using the MNIST dataset by [LeCun et al.](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "The dataset is about recognising handwritten digits (0-9) and contains 70,000 example images.\n",
        "\n",
        "***\n",
        "\n",
        "**Tip:** When implementing a network it's good practice to first test it on one of the many available 'standard datasets'. These datasets are standardised and the attainable performance on them is well known. This ensures that, when your network doesn't learn well (or at all), you know the problem is in the network implementation. \n",
        "\n",
        "If you start off with your own dataset and run into learning problems, it's difficult to ascertain whether your data doesn't contain enough variance for the network to learn anything, or whether the network implementation is incorrect.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkpXY4H4i1y_",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 - Splitting the data\n",
        "\n",
        "At this point you have a dataset and want to train your model. You could train the model on all the data you have, but then how would you know whether the model works well on data not in the training set? In other words, how do you know if the model 'memorised' the training set really well, or if the model has learned useful information about the patterns present in the dataset? Obviously we want the latter: we want the model to *generalise well*. A model that generalises well can handle previously unseen data as well, which translates into good *real-world performance*.\n",
        "\n",
        "Usually you split your dataset into three parts to estimate the model performance:\n",
        "- A large **training set**: this contains the bulk of the data\n",
        "- A **validation set**: you use this during training to evaluate model performance, tune hyperparameters, and spot problems like overfitting.\n",
        "- A **test set**: you use this set at the end of the process, when you've finished training your model, as a final check. \n",
        "    * **Can you explain why the validation set is necessary?**\n",
        "***\n",
        "Run the cells below to load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KOKrnExi1y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    '''Function to load the MNIST data into train/validation sets'''\n",
        "\n",
        "    X_train = np.load('MNIST_data/X_train.npy')\n",
        "    Y_train = np.load('MNIST_data/Y_train.npy')\n",
        "    X_val = np.load('MNIST_data/X_val.npy')\n",
        "    Y_val = np.load('MNIST_data/Y_val.npy')\n",
        "\n",
        "    #normalise data\n",
        "    X_train = X_train / 255.\n",
        "    X_val = X_val / 255.\n",
        "\n",
        "    #Convert labels to one-hot format\n",
        "    Y_train = np.eye(10)[Y_train.reshape(-1)]\n",
        "    Y_val = np.eye(10)[Y_val.reshape(-1)]\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnpZtW8di1zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train, X_val, Y_val = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpmDNhT3i1zD",
        "colab_type": "text"
      },
      "source": [
        "This will download the dataset files to your computer, and read them into memory. It also normalises the data between 0 and 1 (**can you explain why?**), and applies one-hot encoding to the labels.\n",
        "\n",
        "One-hot encoding means that we convert the numerical label into a vector of length equal to the number of classes. The vector is filled with zeros except at the location representing the label in question. Run the cell below to see for yourself what this looks like.\n",
        "\n",
        "We have already converted the images into vectors ('flattened' them). **If you think back to the lecture of this morning, can you explain why this is done?** *Hint: think about the shape of the input layer of the network.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E5iBW-Ci1zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplot(131)\n",
        "plt.imshow(np.reshape(X_train[500], (28, 28)), cmap=plt.get_cmap('gray')) #plot the 501st train set entry\n",
        "print('corresponding label first image:' + str(Y_train[500]))\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(np.reshape(X_train[-1], (28, 28)), cmap=plt.get_cmap('gray')) #plot the last train set entry\n",
        "print('corresponding label middle image:' + str(Y_train[-1]))\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(np.reshape(X_val[0], (28, 28)), cmap=plt.get_cmap('gray')) #plot the first validation set entry\n",
        "print('corresponding label last image:' + str(Y_val[0]))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_tvRPX7i1zH",
        "colab_type": "text"
      },
      "source": [
        "We need this encoding because of the way the network makes decisions on multi-class problems (more than two separate classes). The last layer in the network will have a number of neurons equal to the number of classes (and the size of the one-hot vector!). You can visualise it in the MNIST case as such:\n",
        "\n",
        "![logistic regression](http://www.paulvangent.com/files/DL_Course/day1_images/outputlayer.jpg)\n",
        "\n",
        "In the case of an input image with a '1' on it, we want all the neurons to output an activation value close to zero, *except the second one, which represents the digit '1'*. **Can you explain how the one-hot vector relates to the output?** \n",
        "\n",
        "When classes are mutually exclusive (there is only one correct answer), the output layer often uses a 'softmax' function. The softmax function scales the output vector so that its total sum equals one. Now each neuron in the output layer can be seen as representing the probability of being the correct output. See the image below for a visualisation:\n",
        "\n",
        "![softmax](http://www.paulvangent.com/files/DL_Course/day1_images/softmax_one-hot.png)\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLWiPZKXi1zI",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Building Blocks of Neural Nets\n",
        "\n",
        "As discussed in the lectuers this morning, the basis of the neural net is a single neuron. You can think of a single neuron as a being a special flavor of logistic regression. It takes the available inputs and weighs them, sums them together, adds a bias 'b' term, and computes an activation function (here we used the sigmoid function) to determine the output value.\n",
        "\n",
        "![logistic regression](http://www.paulvangent.com/files/DL_Course/day1_images/logreg1.jpg)\n",
        "\n",
        "Usually a bias term 'b' is added, which quite literally 'biases' the neuron towards or away from activation. When updating the weights during training, the bias term is individually updated as well.\n",
        "\n",
        "'Activation functions' compute the output value of the neuron. An important feature of them is that they add non-linearity to the linear weighting and summation steps. \n",
        "- **Can you explain why this is important?**\n",
        "- **What are the effects of this non-linearity on the type of functions the neuron can fit to the data?**\n",
        "\n",
        "We have provided plots of several popular activation functions here:\n",
        "\n",
        "![Activation Fucntions](http://www.paulvangent.com/files/DL_Course/day1_images/activationfunctions.jpg)\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUb5gRTKi1zI",
        "colab_type": "text"
      },
      "source": [
        "## 4 - Defining a Feed Forward Net in Keras\n",
        "\n",
        "In this function we define a simple two-layer neural net, consisting only of an input layer and an output layer. It will look a little like this:\n",
        "\n",
        "![Simple neural net](http://www.paulvangent.com/files/DL_Course/day1_images/NN1.jpg)\n",
        "\n",
        "Some things to note:\n",
        "- The input layer is the same size as the number of pixels in the MNIST images (28x28 = 784) \n",
        "- The output layer is of size equal to the number of classes in our dataset. \n",
        "    * *The numbers range from 0-9, so the number of classes is 10. This means we have 10 output neurons.*\n",
        "    \n",
        "In the output layer, each of the 10 neurons represents one class. When classifying, the neuron with the highest activation represents the network's decision. For example if in the output layer the first neuron shows the highest activation, the predicted digit is 0.\n",
        "\n",
        "Your task is to add an **input** and an **output** layer to the model below. \n",
        "\n",
        "The **input** layer expects three arguments:\n",
        "- number of input values (in this function, that is **input_dim[0]**)\n",
        "- shape of input (input_dim)\n",
        "- activation function (use 'relu')\n",
        "\n",
        "The **output** layer expects two arguments:\n",
        "- the size of the output (the number of classes)\n",
        "- the activation function (use a softmax function)\n",
        "\n",
        "[Hint](https://keras.io/layers/core/#dense)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Gvzxtbi1zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model(input_dim=(784,), num_classes=10):\n",
        "    '''Function to generate a baseline neural net\n",
        "    \n",
        "    Keyword arguments:\n",
        "    - input_shape: expected dimensions of input data (default = (784,))\n",
        "    - num_classes: total number of categories in dataset (default = 10)\n",
        "    '''\n",
        "    \n",
        "    model = Sequential()\n",
        "    ##Add your code here (Approx. 2 lines)\n",
        "    model.add(None)\n",
        "    model.add(None)\n",
        "    ##End of your code\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y5fhFHoi1zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call the function to build the model\n",
        "model = baseline_model(input_dim=(784,), num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_6leCti1zO",
        "colab_type": "text"
      },
      "source": [
        "# 5 - Training Your Baseline Model\n",
        "\n",
        "So far we:\n",
        "- Loaded a dataset consisting of 65,000 digits into memory, split into 55k training examples and 10k validation examples;\n",
        "    * There's also a test set of 5K examples which we will not use now;\n",
        "    * **Can you explain the importance of a test set?**\n",
        "- Normalised the image data, and converted the label data to one-hot encoding;\n",
        "- Built a baseline neural net consisting of an input layer and an output layer.\n",
        "\n",
        "Now it's time to put our creation to work! To do so, Keras model instances have the 'fit()' function. The fit() function trains the model for a given number of epochs (=full passes through entire training set). You can specify a validation set to evaluate performance after each epoch.\n",
        "\n",
        "**Exercise:** Your task is to fit the model to the data by calling **model.fit()** with the appropriate arguments. Complete the function call below. It will expect:\n",
        "- training samples and training labels\n",
        "- validation samples and labels\n",
        "- number of epochs (use 10 epochs for now)\n",
        "- batch size (use 256 for now)\n",
        "***\n",
        "Hint: [look at the Keras \"model\" API entry](https://keras.io/models/sequential/)  \n",
        "Hint2: [look a little closer](https://keras.io/models/sequential/#fit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtFaBHqii1zO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Your code here (approx. 1 line)\n",
        "model.fit(X_train, Y_train,\n",
        "          validation_data=(X_val, Y_val),\n",
        "          epochs=10,\n",
        "          batch_size=256)\n",
        "##End of your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EM5FnDqi1zQ",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "Train on 55000 samples, validate on 10000 samples  \n",
        "Epoch 1/10  \n",
        "55000/55000 [==============================] - 8s 144us/step - loss: 1.7160 - acc: 0.5975 - val_loss: 1.0110 - val_acc: 0.8023  \n",
        "Epoch 2/10  \n",
        "55000/55000 [==============================] - 7s 128us/step - loss: 0.7537 - acc: 0.8295 - val_loss: 0.5631 - val_acc: 0.8673  \n",
        "Epoch 3/10  \n",
        "55000/55000 [==============================] - 7s 130us/step - loss: 0.5015 - acc: 0.8735 - val_loss: 0.4301 - val_acc: 0.8901  \n",
        ".....   \n",
        "Epoch 10/10  \n",
        "55000/55000 [==============================] - 7s 136us/step - loss: 0.2851 - acc: 0.9179 - val_loss: 0.2727 - val_acc: 0.9202 \n",
        "\n",
        "***\n",
        "\n",
        "**Note** that the actual numbers will vary for your training case, depending on cpu speed, how the network initialises, and the random shuffle of the training data.  \n",
        "\n",
        "***\n",
        "\n",
        "You've now trained a simple network! How well did it perform?\n",
        "\n",
        "In the next section we'll expand the architecture and introduce a hidden layer into the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjf6LamQi1zQ",
        "colab_type": "text"
      },
      "source": [
        "# 6 - Adding a Hidden Layer\n",
        "\n",
        "It's time to add a hidden layer to the network. \n",
        "\n",
        "**Exercise:** Below is the model definition function you wrote earlier. Your task is to add a hidden layer with 1024 units to the function defined below. Use a relu activation function. Put a dropout regularisation layer between the hidden layers and the output layer. Use a dropout rate of 20%.\n",
        "\n",
        "***\n",
        "Hint: [Normal layer](https://keras.io/layers/core/#dense)  \n",
        "Hint2: [Dropout layer](https://keras.io/layers/core/#dropout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX1AdpDLi1zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hiddenlayer_model(input_dim=(784,), num_classes=10):\n",
        "    '''Function to generate a baseline neural net\n",
        "    \n",
        "    Keyword arguments:\n",
        "    - input_shape: expected dimensions of input data (default = (784,))\n",
        "    - num_classes: total number of categories in dataset (default = 10)\n",
        "    '''\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(input_dim[0], input_shape=input_dim, activation='relu'))\n",
        "    ##Add your code here (Approx 2 lines)\n",
        "\n",
        "    \n",
        "    ##End of your code\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFDmYNgXi1zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = hiddenlayer_model()\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m5gBroqi1zU",
        "colab_type": "text"
      },
      "source": [
        "In the output you should see four layers. Two dense, a Dropout, and a Dense at the end.\n",
        "\n",
        "Shapes should be, respectively:  \n",
        "(None, 784)  \n",
        "(None, 1024)  \n",
        "(None, 1024)  \n",
        "(None, 10)  \n",
        "\n",
        "**Can you explain why adding a hidden layer improves our model's capabilities? What is it's effect?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcJMU4Vi1zU",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "The next step is to fit the model again. The process is the same as you did with the neural net before. \n",
        "\n",
        "**Exercise:** Your task is to complete the fit function again. Remember to pass the **model.fit()** function:\n",
        "- training samples and labels\n",
        "- validation samples and labels\n",
        "- number of epochs (use 10)\n",
        "- batch size (use 256)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxnR50Nui1zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Your code here (approx. 1 line)\n",
        "\n",
        "##End of your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ02yHXAi1zX",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "Since you added the hidden layer, did performance increase? **Why do you think this is (not) the case?**\n",
        "\n",
        "**Can you explain the effects of the dropout layer?**\n",
        "\n",
        "**Exercise:** Try removing the dropout layer from the model function. You can either put a '#' in front of the line or delete it. Don't forget to run the cell afterwards (shift enter). After you've done this, complete and the cell below and compare to the training run above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNMenpD_i1zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = hiddenlayer_model()\n",
        "##Your code here (approx. 1 line)\n",
        "model.fit(None)\n",
        "##End of your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqws7mV6i1zb",
        "colab_type": "text"
      },
      "source": [
        "Feel free to play with adding more layers to the model, or with training it for more epochs at this point. When you're finished it's time to look at how the model performs on a different dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7DUCOzYi1zd",
        "colab_type": "text"
      },
      "source": [
        "## 7 - Moving On To Object Recognition\n",
        "\n",
        "We will now apply this model to the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "This dataset contains 60,000 images divided into 10 different classes (hence CIFAR**10**), from inanimate objects (cars, airplanes) to animals.\n",
        "\n",
        "![cifar10](http://www.paulvangent.com/files/DL_Course/day1_images/cifar_10.png)\n",
        "\n",
        "In this exercise we will show you how to load the data and flatten each image into a vector, but you have to do the normalisation and one-hot encoding yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_GNyMTki1ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(X_train, Y_train), (X_val, Y_val) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NopDxHy0i1zi",
        "colab_type": "text"
      },
      "source": [
        "Now we flatten each image so that the resulting array has shape (number_of_images, pixels_per_image). We have colour images of 32x32 pixels, so the shape of each image array is 32x32x3 (3 colour channels for **R**ed **G**reen & **B**lue). To get the size of the flattened image, we thus use 32x32x3 (=3,072).\n",
        "\n",
        "To flatten the image, we use [np.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). Note we can get the shape from the image array with **.shape**. This returns an array with an entry for each dimension on the image. It's in the format *[Number-of-samples, X-pixels, Y-pixels, number-of-colour-channels]*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHnnnAwni1zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We flatten each image in validation and train sets\n",
        "X_train_flat = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2] * X_train.shape[3]))\n",
        "X_val_flat = np.reshape(X_val, (X_val.shape[0], X_val.shape[1] * X_val.shape[2] * X_val.shape[3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNNOihnZi1zk",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Your next exercise is to complete the code below. You need to take two steps:\n",
        "- normalise the images between 0 and 1. Note that the images consist of 8-bit integers, meaning their range is between 0-255.\n",
        "- apply one-hot encoding to the labels. How many classes do we have?\n",
        "\n",
        "[One-hot encoding hint](https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html). \n",
        "You can also look at how we did it earlier in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpkd3rxOi1zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalise each image\n",
        "##Add your code here (approx. 2 lines)\n",
        "X_train_flat = None\n",
        "X_val_flat = None\n",
        "##End of your code\n",
        "\n",
        "\n",
        "#Convert labels to one-hot\n",
        "##Add your code here (approx. 2 lines)\n",
        "Y_train_hot = None\n",
        "Y_val_hot = None\n",
        "##End of your code\n",
        "\n",
        "print('shape of first training example is: ' + str(X_train_flat[0].shape))\n",
        "print('shape of last validation example is: ' + str(X_val_flat[-1].shape))\n",
        "print('shape of first one-hot vector in training is: ' + str(Y_train_hot[0].shape))\n",
        "print('shape of last one-hot vector in validation is: ' + str(Y_val_hot[-1].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlQGgqsvi1zl",
        "colab_type": "text"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "shape of first training example is: (3072,)  \n",
        "shape of last validation example is: (3072,)  \n",
        "shape of first one-hot vector in training is: (10,)  \n",
        "shape of last one-hot vector in validation is: (10,)  \n",
        "\n",
        "***\n",
        "\n",
        "Now we call the hiddenlayer_model you wrote earlier to initialise a fresh model. We use input_size of 3072 (32x32x3), and 10 classes.\n",
        "\n",
        "**Exercise:** Initialise the model by completing and running the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF70eKU-i1zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Add your code here (approx. 1 line)\n",
        "model = hiddenlayer_model(None, None)\n",
        "##End of your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "KsrBElXni1zo",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "Now fit the model. Fill in the blanks below, remember to feed the model.fit() function:\n",
        "- X and Y from the training data\n",
        "- X and Y from the validation data\n",
        "- set the number of epochs to 10\n",
        "- Batch size of 256 is advised.\n",
        "- In stead of just calling model.fit, we will add \"history = \" before the fit function. You will see why afterwards.\n",
        "\n",
        "**hint:** If you're stuck, you can refer up to the earlier time we called model.fit()\n",
        "\n",
        "Once fitting starts, go get a cup of coffee! This will take ~10-30 minutes depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGlBW4cLi1zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Add your code here (approx. 1 line)\n",
        "history = model.fit(None)\n",
        "##End of your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GzHttMNi1zq",
        "colab_type": "text"
      },
      "source": [
        "****\n",
        "**Expected Output:**\n",
        "\n",
        "Train on 50000 samples, validate on 10000 samples  \n",
        "Epoch 1/10  \n",
        "50000/50000 [==============================] - 5s 109us/step - loss: 2.4346 - acc: 0.2858 - val_loss: 1.7262 - val_acc: 0.3769  \n",
        "Epoch 2/10  \n",
        "50000/50000 [==============================] - 5s 95us/step - loss: 1.7014 - acc: 0.3898 - val_loss: 1.6754 - val_acc: 0.4025  \n",
        "[...]    \n",
        "Epoch 10/10  \n",
        "50000/50000 [==============================] - 5s 93us/step - loss: 1.3486 - acc: 0.5186 - val_loss: 1.4063 - val_acc: 0.5060  \n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7jBFaGbi1zr",
        "colab_type": "text"
      },
      "source": [
        "Remember how we added \"history =\" before model.fit? We did this because, by default, the Keras fit function returns a history object containing information about the training phase of the model. This will help us gauge performance, spot possible overfitting, as well as help with model selection.\n",
        "\n",
        "To visualise, run the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rj192XTi1zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(9,6))\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.title('training and validation loss')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(history.history['acc'], label='training accuracy')\n",
        "plt.plot(history.history['val_acc'], label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.title('training and validation accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0-SyKROi1zu",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "Another great way of visualising model performance is a so-called **confusion plot**. It plots predicted labels against actual labels. \n",
        "\n",
        "We've provided you with the confusion matrix for our version of the feed forward network trained on CIFAR-10. It displays the prediction performance on the training set.\n",
        "\n",
        "Take a look at the plot:\n",
        "\n",
        "![confusion plot](http://www.paulvangent.com/files/DL_Course/day1_images/confusionmatrix_cifar10.png)\n",
        "\n",
        "Already some pretty interesting patterns are visible! Cars and trucks are confused, which makes sense. In general, the inanimate objects (airplane, automobile, ship, truck) are confused with each other more often than the animals in the set. \n",
        "\n",
        "**What do you think, is this indicative of the model learning rules that would help it to generalise?** Incorrect results can often give a lot more information than correct ones.\n",
        "\n",
        "For more information on how to make your own confusion plot, [take a look here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "mYCO3pkYi1zu",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## 8 - Wrapping up\n",
        "Our simple neural network with two hidden layers doesn't work very well for the CIFAR data. After 10 epochs our version of the model reaches ~50% accuracy. While a lot higher than the ~10% chance level we would expect if the network learned absolutely nothing from the data, it still means that **half the time** our prediction is **dead wrong**.\n",
        "\n",
        "Could training longer be an option? To illustrate, we let the model run a little longer (50 epochs) on a powerful GPU, but the model only ended up overfitting. As displayed on the plot below, the signs of overfitting are there. You can spot overfitting:\n",
        "- The training accuracy keeps increasing but the validation accuracy flattens off: the model isn't learning anything new that helps it generalise, it only learns to fit the training set better. **Can you explain why that is bad? Is it always bad?**\n",
        "- Similarly, the training loss keeps decreasing but the validation loss flattens off, and even starts increasing again later on. **Can you explain how this relates to overfitting?**\n",
        "\n",
        "![50 epochs](http://www.paulvangent.com/files/DL_Course/day1_images/NN_plot1.jpg)\n",
        "\n",
        "***\n",
        "**Maybe adding more layers might work?** As an example, we added 5 more hidden layers of a larger size (2048 neurons each) with regularisation layers in between, and let the model run for 50 epochs. The behaviour is very similar to what is displayed above, indicating this is not a sound strategy:\n",
        "\n",
        "![50 epochs with bigger network](http://www.paulvangent.com/files/DL_Course/day1_images/NN_plots_biggernet2.png)\n",
        "***\n",
        "We might even try... \n",
        "![really?](http://www.paulvangent.com/files/DL_Course/day1_images/layers.jpg)\n",
        "\n",
        "But then we run into serious issues with available computation power and memory, and it likely will not improve performance. Clearly slapping more layers onto the model is not the correct approach.\n",
        "\n",
        "It's time to dig a little deeper.\n",
        "\n",
        "![deeeeeeeeeeeeeeper](http://www.paulvangent.com/files/DL_Course/day1_images/deeper.jpg)\n",
        "\n",
        "Much deeper.\n",
        "\n",
        "\n",
        "- **When adding more layers the network becomes more complex, what is the effect on overfitting? Can you explain?**\n",
        "- **How can we counteract the tendency to overfit?**\n",
        "- **What will the effect of regularisation be?**\n",
        "\n",
        "***\n",
        "For more information on how the learning rate affects the network fitting behaviour, take a look [at this link](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AKXlFEL8i1zv",
        "colab_type": "text"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Congratulations on making it this far! In the next exercise notebook we will be looking at implementing a **convolutional neural network**, and applying it to CIFAR-10 again to see if it works any better than a standard feedforward network.\n",
        "\n",
        "In stead of adding more layers, we will be adding smarter layers.\n",
        "\n",
        "[Click here to go to it](Exercise 2 - Convolutional Neural Nets.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyhoOFRci1zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}